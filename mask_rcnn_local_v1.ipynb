{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c434cc-a717-469e-bbde-f0a1efe2d476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy-free training script - avoiding all numpy operations\n",
      "PyTorch version: 2.0.1\n",
      "Torchvision version: 0.15.2\n",
      "Starting NumPy-free training...\n",
      "Using device: cpu\n",
      "Loading dataset...\n",
      "Found 1411 images\n",
      "Train samples: 80, Val samples: 20\n",
      "Loading model...\n",
      "Starting training...\n",
      "\n",
      "=== Epoch 1/3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|▍                                  | 1/80 [00:03<04:27,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0, Loss: 297.9584\n",
      "  loss_classifier: 3.2347\n",
      "  loss_box_reg: 0.0282\n",
      "  loss_mask: 293.4094\n",
      "  loss_objectness: 1.2649\n",
      "  loss_rpn_box_reg: 0.0213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  14%|████▋                             | 11/80 [01:25<08:40,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 10, Loss: 459.3501\n",
      "  loss_classifier: 3.1997\n",
      "  loss_box_reg: 0.0469\n",
      "  loss_mask: 455.7702\n",
      "  loss_objectness: 0.3133\n",
      "  loss_rpn_box_reg: 0.0199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  26%|████████▉                         | 21/80 [02:32<06:24,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 20, Loss: 23.6007\n",
      "  loss_classifier: 3.0722\n",
      "  loss_box_reg: 0.5894\n",
      "  loss_mask: 10.2417\n",
      "  loss_objectness: 8.2558\n",
      "  loss_rpn_box_reg: 1.4417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  39%|█████████████▏                    | 31/80 [04:04<07:19,  8.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 30, Loss: 8.7695\n",
      "  loss_classifier: 2.8151\n",
      "  loss_box_reg: 0.7151\n",
      "  loss_mask: -2.4772\n",
      "  loss_objectness: 6.5069\n",
      "  loss_rpn_box_reg: 1.2096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  51%|█████████████████▍                | 41/80 [05:07<04:05,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 40, Loss: -31.2400\n",
      "  loss_classifier: 2.7692\n",
      "  loss_box_reg: 0.4237\n",
      "  loss_mask: -41.1809\n",
      "  loss_objectness: 6.3051\n",
      "  loss_rpn_box_reg: 0.4429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  64%|█████████████████████▋            | 51/80 [05:50<01:52,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 50, Loss: -113.2122\n",
      "  loss_classifier: 2.7614\n",
      "  loss_box_reg: 0.0762\n",
      "  loss_mask: -122.1666\n",
      "  loss_objectness: 5.7960\n",
      "  loss_rpn_box_reg: 0.3208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  76%|█████████████████████████▉        | 61/80 [06:42<01:36,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 60, Loss: -125.5990\n",
      "  loss_classifier: 2.7308\n",
      "  loss_box_reg: 0.6818\n",
      "  loss_mask: -134.7440\n",
      "  loss_objectness: 4.9005\n",
      "  loss_rpn_box_reg: 0.8319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:  89%|██████████████████████████████▏   | 71/80 [07:57<01:03,  7.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 70, Loss: 14.8977\n",
      "  loss_classifier: 2.6873\n",
      "  loss_box_reg: 0.5076\n",
      "  loss_mask: 6.8761\n",
      "  loss_objectness: 4.1534\n",
      "  loss_rpn_box_reg: 0.6734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████████████████████████████| 80/80 [08:57<00:00,  6.72s/it]\n",
      "Evaluating:   5%|█▌                              | 1/20 [00:12<03:49, 12.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in evaluation batch 0: 'list' object has no attribute 'values'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # Use non-interactive backend\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Avoid importing numpy directly - use torch operations instead\n",
    "print(\"NumPy-free training script - avoiding all numpy operations\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "\n",
    "\n",
    "class iSAIDDataset(Dataset):\n",
    "    def __init__(self, root_dir, transforms=None):\n",
    "        \"\"\"\n",
    "        iSAID Dataset for Mask R-CNN training without NumPy dependencies\n",
    "        \"\"\"\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # Paths to different directories\n",
    "        self.instance_masks_dir = self.root_dir / \"Instance_masks\" / \"images\"\n",
    "        self.semantic_masks_dir = self.root_dir / \"Semantic_masks\" / \"images\"\n",
    "        \n",
    "        # Check if directories exist\n",
    "        if not self.instance_masks_dir.exists():\n",
    "            raise ValueError(f\"Instance masks directory not found: {self.instance_masks_dir}\")\n",
    "        if not self.semantic_masks_dir.exists():\n",
    "            raise ValueError(f\"Semantic masks directory not found: {self.semantic_masks_dir}\")\n",
    "        \n",
    "        # Get all image files\n",
    "        self.image_files = sorted([f for f in os.listdir(self.instance_masks_dir) \n",
    "                                 if f.endswith('.png')])\n",
    "        \n",
    "        print(f\"Found {len(self.image_files)} images\")\n",
    "        \n",
    "        # iSAID class mapping (15 classes + background)\n",
    "        self.classes = [\n",
    "            'background', 'ship', 'storage_tank', 'baseball_diamond', \n",
    "            'tennis_court', 'basketball_court', 'ground_track_field',\n",
    "            'bridge', 'large_vehicle', 'small_vehicle', 'helicopter',\n",
    "            'swimming_pool', 'roundabout', 'soccer_ball_field', \n",
    "            'plane', 'harbor'\n",
    "        ]\n",
    "        \n",
    "        # RGB to class mapping as nested dictionary\n",
    "        self.rgb_to_class = {\n",
    "            0: {0: {0: 0, 63: 1, 127: 9, 191: 10, 255: 11}},  # R=0\n",
    "            # Add more mappings as needed\n",
    "        }\n",
    "        \n",
    "        # Keep track of failed samples\n",
    "        self.failed_samples = set()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Skip failed samples\n",
    "        if idx in self.failed_samples:\n",
    "            return self.get_dummy_sample(idx)\n",
    "        \n",
    "        try:\n",
    "            # Get image filename\n",
    "            img_filename = self.image_files[idx]\n",
    "            \n",
    "            # Create a dummy RGB image\n",
    "            img = Image.new('RGB', (800, 800), color=(128, 128, 128))\n",
    "            \n",
    "            # Load instance mask using PIL only\n",
    "            instance_mask_path = self.instance_masks_dir / img_filename\n",
    "            if not instance_mask_path.exists():\n",
    "                raise FileNotFoundError(f\"Instance mask not found: {instance_mask_path}\")\n",
    "            \n",
    "            # Load masks as PIL images and convert to tensors directly\n",
    "            instance_pil = Image.open(instance_mask_path)\n",
    "            if instance_pil.mode != 'L':\n",
    "                instance_pil = instance_pil.convert('L')\n",
    "            \n",
    "            # Load semantic mask\n",
    "            semantic_filename = img_filename.replace('_instance_id_RGB.png', '_instance_color_RGB.png')\n",
    "            semantic_mask_path = self.semantic_masks_dir / semantic_filename\n",
    "            \n",
    "            if not semantic_mask_path.exists():\n",
    "                raise FileNotFoundError(f\"Semantic mask not found: {semantic_mask_path}\")\n",
    "            \n",
    "            semantic_pil = Image.open(semantic_mask_path)\n",
    "            if semantic_pil.mode != 'RGB':\n",
    "                semantic_pil = semantic_pil.convert('RGB')\n",
    "            \n",
    "            # Resize image to match mask dimensions\n",
    "            mask_width, mask_height = instance_pil.size\n",
    "            img = img.resize((mask_width, mask_height))\n",
    "            \n",
    "            # Convert PIL to tensors directly (avoiding numpy)\n",
    "            instance_tensor = transforms.ToTensor()(instance_pil)\n",
    "            semantic_tensor = transforms.ToTensor()(semantic_pil)\n",
    "            \n",
    "            # Extract instances using pure PyTorch operations\n",
    "            boxes, labels, masks = self.extract_instances_torch(instance_tensor, semantic_tensor)\n",
    "            \n",
    "            # Handle empty detections\n",
    "            if len(boxes) == 0:\n",
    "                boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                labels = torch.zeros((0,), dtype=torch.int64)\n",
    "                masks = torch.zeros((0, mask_height, mask_width), dtype=torch.uint8)\n",
    "                area = torch.zeros((0,), dtype=torch.float32)\n",
    "                iscrowd = torch.zeros((0,), dtype=torch.int64)\n",
    "            else:\n",
    "                boxes = torch.stack(boxes)\n",
    "                labels = torch.tensor(labels, dtype=torch.int64)\n",
    "                masks = torch.stack(masks)\n",
    "                area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "                iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "            \n",
    "            image_id = torch.tensor([idx])\n",
    "            \n",
    "            target = {\n",
    "                \"boxes\": boxes,\n",
    "                \"labels\": labels,\n",
    "                \"masks\": masks,\n",
    "                \"image_id\": image_id,\n",
    "                \"area\": area,\n",
    "                \"iscrowd\": iscrowd\n",
    "            }\n",
    "            \n",
    "            if self.transforms:\n",
    "                img = self.transforms(img)\n",
    "            else:\n",
    "                img = transforms.ToTensor()(img)\n",
    "                \n",
    "            return img, target\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample {idx}: {str(e)}\")\n",
    "            self.failed_samples.add(idx)\n",
    "            return self.get_dummy_sample(idx)\n",
    "    \n",
    "    def get_dummy_sample(self, idx):\n",
    "        \"\"\"Return a valid dummy sample\"\"\"\n",
    "        dummy_img = torch.zeros((3, 800, 800), dtype=torch.float32)\n",
    "        dummy_target = {\n",
    "            \"boxes\": torch.zeros((0, 4), dtype=torch.float32),\n",
    "            \"labels\": torch.zeros((0,), dtype=torch.int64),\n",
    "            \"masks\": torch.zeros((0, 800, 800), dtype=torch.uint8),\n",
    "            \"image_id\": torch.tensor([idx]),\n",
    "            \"area\": torch.zeros((0,), dtype=torch.float32),\n",
    "            \"iscrowd\": torch.zeros((0,), dtype=torch.int64)\n",
    "        }\n",
    "        return dummy_img, dummy_target\n",
    "    \n",
    "    def extract_instances_torch(self, instance_tensor, semantic_tensor):\n",
    "        \"\"\"Extract instances using pure PyTorch operations\"\"\"\n",
    "        try:\n",
    "            # instance_tensor shape: (1, H, W) or (H, W)\n",
    "            if instance_tensor.dim() == 3:\n",
    "                instance_mask = instance_tensor[0]  # Take first channel\n",
    "            else:\n",
    "                instance_mask = instance_tensor\n",
    "            \n",
    "            # Get unique values using PyTorch\n",
    "            unique_vals = torch.unique(instance_mask)\n",
    "            unique_vals = unique_vals[unique_vals > 0]  # Remove background\n",
    "            \n",
    "            boxes = []\n",
    "            labels = []\n",
    "            masks = []\n",
    "            \n",
    "            for instance_id in unique_vals:\n",
    "                # Create binary mask\n",
    "                binary_mask = (instance_mask == instance_id).float()\n",
    "                \n",
    "                # Skip small masks\n",
    "                if binary_mask.sum() < 100:\n",
    "                    continue\n",
    "                \n",
    "                # Get bounding box using torch operations\n",
    "                nonzero_indices = torch.nonzero(binary_mask, as_tuple=False)\n",
    "                if len(nonzero_indices) == 0:\n",
    "                    continue\n",
    "                \n",
    "                y_coords = nonzero_indices[:, 0]\n",
    "                x_coords = nonzero_indices[:, 1]\n",
    "                \n",
    "                xmin = x_coords.min().float()\n",
    "                xmax = x_coords.max().float()\n",
    "                ymin = y_coords.min().float()\n",
    "                ymax = y_coords.max().float()\n",
    "                \n",
    "                # Skip invalid boxes\n",
    "                if xmax <= xmin or ymax <= ymin or (xmax - xmin) < 5 or (ymax - ymin) < 5:\n",
    "                    continue\n",
    "                \n",
    "                boxes.append(torch.tensor([xmin, ymin, xmax, ymax], dtype=torch.float32))\n",
    "                \n",
    "                # Simple class assignment (you can improve this)\n",
    "                class_label = self.get_class_torch(semantic_tensor, binary_mask)\n",
    "                labels.append(class_label)\n",
    "                \n",
    "                # Convert mask to uint8\n",
    "                mask_uint8 = (binary_mask * 255).byte()\n",
    "                masks.append(mask_uint8)\n",
    "            \n",
    "            return boxes, labels, masks\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting instances with torch: {str(e)}\")\n",
    "            return [], [], []\n",
    "    \n",
    "    def get_class_torch(self, semantic_tensor, instance_mask):\n",
    "        \"\"\"Get class using PyTorch operations\"\"\"\n",
    "        try:\n",
    "            # semantic_tensor shape: (3, H, W)\n",
    "            # instance_mask shape: (H, W)\n",
    "            \n",
    "            # Get pixels where instance mask is active\n",
    "            mask_indices = instance_mask > 0\n",
    "            if not mask_indices.any():\n",
    "                return 1\n",
    "            \n",
    "            # Sample a few pixels from the semantic mask\n",
    "            semantic_pixels = semantic_tensor[:, mask_indices]  # Shape: (3, N)\n",
    "            \n",
    "            # Simple heuristic: use the mean RGB values\n",
    "            if semantic_pixels.size(1) > 0:\n",
    "                mean_rgb = semantic_pixels.mean(dim=1)  # Shape: (3,)\n",
    "                \n",
    "                # Map RGB to class (simplified)\n",
    "                r, g, b = mean_rgb[0].item(), mean_rgb[1].item(), mean_rgb[2].item()\n",
    "                \n",
    "                # Simple color-based classification\n",
    "                if r > 0.5:  # Red-ish\n",
    "                    return 1  # ship\n",
    "                elif g > 0.5:  # Green-ish\n",
    "                    return 2  # storage_tank\n",
    "                elif b > 0.5:  # Blue-ish\n",
    "                    return 3  # baseball_diamond\n",
    "                else:\n",
    "                    return min(int(r * 15) + 1, 15)  # Use red channel for class\n",
    "            \n",
    "            return 1  # Default class\n",
    "            \n",
    "        except Exception as e:\n",
    "            return 1\n",
    "\n",
    "\n",
    "def get_model(num_classes):\n",
    "    \"\"\"Create Mask R-CNN model\"\"\"\n",
    "    model = maskrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for DataLoader\"\"\"\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if len(batch) == 0:\n",
    "        return [], []\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for i, (images, targets) in enumerate(tqdm(data_loader, desc=f\"Epoch {epoch}\")):\n",
    "        try:\n",
    "            if len(images) == 0:\n",
    "                continue\n",
    "                \n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            # Forward pass\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            \n",
    "            # Check for invalid loss\n",
    "            if torch.isnan(losses) or torch.isinf(losses):\n",
    "                print(f\"Invalid loss detected, skipping batch {i}\")\n",
    "                continue\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += losses.item()\n",
    "            num_batches += 1\n",
    "            \n",
    "            if i % print_freq == 0:\n",
    "                print(f\"Epoch: {epoch}, Batch: {i}, Loss: {losses.item():.4f}\")\n",
    "                for k, v in loss_dict.items():\n",
    "                    print(f\"  {k}: {v.item():.4f}\")\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {i}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return running_loss / max(num_batches, 1)\n",
    "\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (images, targets) in enumerate(tqdm(data_loader, desc=\"Evaluating\")):\n",
    "            try:\n",
    "                if len(images) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                images = list(image.to(device) for image in images)\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "                \n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                \n",
    "                if not (torch.isnan(losses) or torch.isinf(losses)):\n",
    "                    total_loss += losses.item()\n",
    "                    num_batches += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in evaluation batch {i}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    return total_loss / max(num_batches, 1)\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Starting NumPy-free training...\")\n",
    "    \n",
    "    # Configuration\n",
    "    config = {\n",
    "        'data_dir': '/Users/soumendusekharbhattacharjee/Downloads/iSAID_data/train',\n",
    "        'batch_size': 1,\n",
    "        'num_epochs': 3,\n",
    "        'learning_rate': 0.001,\n",
    "        'weight_decay': 0.0005,\n",
    "        'num_workers': 0,\n",
    "        'save_dir': './checkpoints',\n",
    "        'num_classes': 16\n",
    "    }\n",
    "    \n",
    "    # Create save directory\n",
    "    os.makedirs(config['save_dir'], exist_ok=True)\n",
    "    \n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Data transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        # Dataset and DataLoader\n",
    "        print(\"Loading dataset...\")\n",
    "        dataset = iSAIDDataset(config['data_dir'], transforms=transform)\n",
    "        \n",
    "        if len(dataset) == 0:\n",
    "            raise ValueError(\"No valid samples found in dataset\")\n",
    "        \n",
    "        # Split dataset\n",
    "        total_size = min(len(dataset), 100)\n",
    "        train_size = int(0.8 * total_size)\n",
    "        val_size = total_size - train_size\n",
    "        \n",
    "        indices = list(range(total_size))\n",
    "        train_indices = indices[:train_size]\n",
    "        val_indices = indices[train_size:total_size]\n",
    "        \n",
    "        train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "        val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "        \n",
    "        print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=True,\n",
    "            num_workers=config['num_workers'],\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=config['num_workers'],\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        # Model\n",
    "        print(\"Loading model...\")\n",
    "        model = get_model(config['num_classes'])\n",
    "        model.to(device)\n",
    "        \n",
    "        # Optimizer\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = optim.SGD(params, lr=config['learning_rate'], \n",
    "                             momentum=0.9, weight_decay=config['weight_decay'])\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "        \n",
    "        print(\"Starting training...\")\n",
    "        \n",
    "        # Training loop\n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        \n",
    "        for epoch in range(config['num_epochs']):\n",
    "            print(f\"\\n=== Epoch {epoch+1}/{config['num_epochs']} ===\")\n",
    "            \n",
    "            # Train\n",
    "            train_loss = train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            # Validate\n",
    "            val_loss = evaluate(model, val_loader, device)\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            # Update learning rate\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "            }\n",
    "            torch.save(checkpoint, f\"{config['save_dir']}/maskrcnn_epoch_{epoch}.pth\")\n",
    "            print(f\"Checkpoint saved: maskrcnn_epoch_{epoch}.pth\")\n",
    "        \n",
    "        print(\"Training completed successfully!\")\n",
    "        print(f\"Failed samples: {len(dataset.failed_samples)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during training: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218d0e46-29cb-4a14-b98b-dd3fe5b17cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Rectangle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Ensure numpy is properly imported\n",
    "try:\n",
    "    import numpy as np\n",
    "    print(f\"NumPy version: {np.__version__}\")\n",
    "except ImportError as e:\n",
    "    print(f\"NumPy import error: {e}\")\n",
    "    raise ImportError(\"NumPy is required but not available. Please install it with: pip install numpy\")\n",
    "\n",
    "\n",
    "class iSAIDDataset(Dataset):\n",
    "    def __init__(self, root_dir, transforms=None):\n",
    "        \"\"\"\n",
    "        iSAID Dataset for Mask R-CNN validation\n",
    "        \"\"\"\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        # Paths to different directories\n",
    "        self.instance_masks_dir = self.root_dir / \"Instance_masks\" / \"images\"\n",
    "        self.semantic_masks_dir = self.root_dir / \"Semantic_masks\" / \"images\"\n",
    "        \n",
    "        # Check if directories exist\n",
    "        if not self.instance_masks_dir.exists():\n",
    "            raise ValueError(f\"Instance masks directory not found: {self.instance_masks_dir}\")\n",
    "        if not self.semantic_masks_dir.exists():\n",
    "            raise ValueError(f\"Semantic masks directory not found: {self.semantic_masks_dir}\")\n",
    "        \n",
    "        # Get all image files\n",
    "        self.image_files = sorted([f for f in os.listdir(self.instance_masks_dir) \n",
    "                                 if f.endswith('.png')])\n",
    "        \n",
    "        # iSAID class mapping (15 classes + background)\n",
    "        self.classes = [\n",
    "            'background', 'ship', 'storage_tank', 'baseball_diamond', \n",
    "            'tennis_court', 'basketball_court', 'ground_track_field',\n",
    "            'bridge', 'large_vehicle', 'small_vehicle', 'helicopter',\n",
    "            'swimming_pool', 'roundabout', 'soccer_ball_field', \n",
    "            'plane', 'harbor'\n",
    "        ]\n",
    "        \n",
    "        # Class colors for visualization\n",
    "        self.class_colors = [\n",
    "            (0, 0, 0),        # background (black)\n",
    "            (255, 0, 0),      # ship (red)\n",
    "            (0, 255, 0),      # storage_tank (green)\n",
    "            (0, 0, 255),      # baseball_diamond (blue)\n",
    "            (255, 255, 0),    # tennis_court (yellow)\n",
    "            (255, 0, 255),    # basketball_court (magenta)\n",
    "            (0, 255, 255),    # ground_track_field (cyan)\n",
    "            (128, 0, 0),      # bridge (dark red)\n",
    "            (0, 128, 0),      # large_vehicle (dark green)\n",
    "            (0, 0, 128),      # small_vehicle (dark blue)\n",
    "            (128, 128, 0),    # helicopter (olive)\n",
    "            (128, 0, 128),    # swimming_pool (purple)\n",
    "            (0, 128, 128),    # roundabout (teal)\n",
    "            (255, 128, 0),    # soccer_ball_field (orange)\n",
    "            (255, 0, 128),    # plane (pink)\n",
    "            (128, 255, 0),    # harbor (lime)\n",
    "        ]\n",
    "        \n",
    "        # Create RGB to class mapping\n",
    "        self.rgb_to_class = self.create_rgb_mapping()\n",
    "        \n",
    "    def create_rgb_mapping(self):\n",
    "        \"\"\"Create RGB to class index mapping for iSAID\"\"\"\n",
    "        rgb_mapping = {\n",
    "            (0, 0, 0): 0,        # background\n",
    "            (0, 0, 63): 1,       # ship\n",
    "            (0, 63, 63): 2,      # storage_tank\n",
    "            (0, 63, 0): 3,       # baseball_diamond\n",
    "            (0, 63, 127): 4,     # tennis_court\n",
    "            (0, 63, 191): 5,     # basketball_court\n",
    "            (0, 63, 255): 6,     # ground_track_field\n",
    "            (0, 127, 63): 7,     # bridge\n",
    "            (0, 127, 127): 8,    # large_vehicle\n",
    "            (0, 0, 127): 9,      # small_vehicle\n",
    "            (0, 0, 191): 10,     # helicopter\n",
    "            (0, 0, 255): 11,     # swimming_pool\n",
    "            (0, 191, 127): 12,   # roundabout\n",
    "            (0, 127, 191): 13,   # soccer_ball_field\n",
    "            (0, 127, 255): 14,   # plane\n",
    "            (0, 100, 155): 15,   # harbor\n",
    "        }\n",
    "        return rgb_mapping\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            # Get image filename\n",
    "            img_filename = self.image_files[idx]\n",
    "            base_name = img_filename.replace('_instance_id_RGB.png', '')\n",
    "            \n",
    "            # Create a more realistic dummy RGB image\n",
    "            # In practice, load your actual RGB images here\n",
    "            img = self.create_dummy_image(base_name)\n",
    "            \n",
    "            # Load instance mask\n",
    "            instance_mask_path = self.instance_masks_dir / img_filename\n",
    "            instance_mask = np.array(Image.open(instance_mask_path))\n",
    "            \n",
    "            # Load semantic mask\n",
    "            semantic_filename = img_filename.replace('_instance_id_RGB.png', '_instance_color_RGB.png')\n",
    "            semantic_mask_path = self.semantic_masks_dir / semantic_filename\n",
    "            semantic_mask = np.array(Image.open(semantic_mask_path))\n",
    "            \n",
    "            # Resize image to match mask dimensions\n",
    "            mask_height, mask_width = instance_mask.shape[:2]\n",
    "            img = img.resize((mask_width, mask_height))\n",
    "            \n",
    "            # Extract ground truth instances\n",
    "            boxes, labels, masks = self.extract_instances(instance_mask, semantic_mask)\n",
    "            \n",
    "            # Handle empty detections\n",
    "            if len(boxes) == 0:\n",
    "                boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                labels = torch.zeros((0,), dtype=torch.int64)\n",
    "                masks = torch.zeros((0, mask_height, mask_width), dtype=torch.uint8)\n",
    "                area = torch.zeros((0,), dtype=torch.float32)\n",
    "                iscrowd = torch.zeros((0,), dtype=torch.int64)\n",
    "            else:\n",
    "                boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "                labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "                masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "                area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "                iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)\n",
    "            \n",
    "            image_id = torch.tensor([idx])\n",
    "            \n",
    "            target = {\n",
    "                \"boxes\": boxes,\n",
    "                \"labels\": labels,\n",
    "                \"masks\": masks,\n",
    "                \"image_id\": image_id,\n",
    "                \"area\": area,\n",
    "                \"iscrowd\": iscrowd\n",
    "            }\n",
    "            \n",
    "            # Store original image for visualization\n",
    "            original_img = np.array(img)\n",
    "            \n",
    "            if self.transforms:\n",
    "                img = self.transforms(img)\n",
    "            else:\n",
    "                img = transforms.ToTensor()(img)\n",
    "                \n",
    "            return img, target, original_img, img_filename\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample {idx}: {str(e)}\")\n",
    "            # Return dummy data\n",
    "            dummy_img = torch.zeros((3, 800, 800), dtype=torch.float32)\n",
    "            dummy_target = {\n",
    "                \"boxes\": torch.zeros((0, 4), dtype=torch.float32),\n",
    "                \"labels\": torch.zeros((0,), dtype=torch.int64),\n",
    "                \"masks\": torch.zeros((0, 800, 800), dtype=torch.uint8),\n",
    "                \"image_id\": torch.tensor([idx]),\n",
    "                \"area\": torch.zeros((0,), dtype=torch.float32),\n",
    "                \"iscrowd\": torch.zeros((0,), dtype=torch.int64)\n",
    "            }\n",
    "            dummy_original = np.zeros((800, 800, 3), dtype=np.uint8)\n",
    "            return dummy_img, dummy_target, dummy_original, f\"dummy_{idx}.png\"\n",
    "    \n",
    "    def create_dummy_image(self, base_name):\n",
    "        \"\"\"Create a more realistic dummy image\"\"\"\n",
    "        # Create a gradient background\n",
    "        width, height = 800, 800\n",
    "        img = Image.new('RGB', (width, height))\n",
    "        pixels = img.load()\n",
    "        \n",
    "        for i in range(width):\n",
    "            for j in range(height):\n",
    "                # Create a simple gradient pattern\n",
    "                r = int(100 + 50 * np.sin(i * 0.01))\n",
    "                g = int(120 + 30 * np.cos(j * 0.01))\n",
    "                b = int(140 + 20 * np.sin((i + j) * 0.005))\n",
    "                pixels[i, j] = (r, g, b)\n",
    "        \n",
    "        return img\n",
    "    \n",
    "    def extract_instances(self, instance_mask, semantic_mask):\n",
    "        \"\"\"Extract individual instances from masks\"\"\"\n",
    "        try:\n",
    "            if len(instance_mask.shape) == 3:\n",
    "                instance_mask = instance_mask[:, :, 0]\n",
    "            \n",
    "            unique_instances = np.unique(instance_mask)\n",
    "            unique_instances = unique_instances[unique_instances > 0]\n",
    "            \n",
    "            boxes = []\n",
    "            labels = []\n",
    "            masks = []\n",
    "            \n",
    "            for instance_id in unique_instances:\n",
    "                instance_binary_mask = (instance_mask == instance_id).astype(np.uint8)\n",
    "                \n",
    "                if np.sum(instance_binary_mask) < 100:\n",
    "                    continue\n",
    "                    \n",
    "                pos = np.where(instance_binary_mask)\n",
    "                if len(pos[0]) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                xmin = int(np.min(pos[1]))\n",
    "                xmax = int(np.max(pos[1]))\n",
    "                ymin = int(np.min(pos[0]))\n",
    "                ymax = int(np.max(pos[0]))\n",
    "                \n",
    "                if xmax <= xmin or ymax <= ymin or (xmax - xmin) < 5 or (ymax - ymin) < 5:\n",
    "                    continue\n",
    "                \n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "                \n",
    "                class_label = self.get_class_from_semantic_mask(semantic_mask, instance_binary_mask)\n",
    "                labels.append(class_label)\n",
    "                masks.append(instance_binary_mask)\n",
    "            \n",
    "            return boxes, labels, masks\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting instances: {str(e)}\")\n",
    "            return [], [], []\n",
    "    \n",
    "    def get_class_from_semantic_mask(self, semantic_mask, instance_mask):\n",
    "        \"\"\"Get class label from semantic mask\"\"\"\n",
    "        try:\n",
    "            masked_semantic = semantic_mask[instance_mask > 0]\n",
    "            \n",
    "            if len(masked_semantic) == 0:\n",
    "                return 1\n",
    "            \n",
    "            if len(semantic_mask.shape) == 3:\n",
    "                rgb_values = masked_semantic.reshape(-1, 3)\n",
    "                unique_colors, counts = np.unique(rgb_values, axis=0, return_counts=True)\n",
    "                most_common_color = tuple(unique_colors[np.argmax(counts)])\n",
    "                return self.rgb_to_class.get(most_common_color, 1)\n",
    "            else:\n",
    "                unique_values, counts = np.unique(masked_semantic, return_counts=True)\n",
    "                most_common_value = unique_values[np.argmax(counts)]\n",
    "                return min(int(most_common_value), 15)\n",
    "                \n",
    "        except Exception as e:\n",
    "            return 1\n",
    "\n",
    "\n",
    "def get_model(num_classes, checkpoint_path=None):\n",
    "    \"\"\"Load trained Mask R-CNN model\"\"\"\n",
    "    model = maskrcnn_resnet50_fpn(pretrained=True)\n",
    "    \n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, num_classes)\n",
    "    \n",
    "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading checkpoint from {checkpoint_path}\")\n",
    "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded model from epoch {checkpoint.get('epoch', 'unknown')}\")\n",
    "    else:\n",
    "        print(\"No checkpoint provided, using pre-trained weights only\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function\"\"\"\n",
    "    batch = [b for b in batch if b is not None]\n",
    "    if len(batch) == 0:\n",
    "        return [], [], [], []\n",
    "    \n",
    "    images, targets, original_imgs, filenames = zip(*batch)\n",
    "    return list(images), list(targets), list(original_imgs), list(filenames)\n",
    "\n",
    "\n",
    "def safe_tensor_to_numpy(tensor):\n",
    "    \"\"\"Safely convert tensor to numpy array\"\"\"\n",
    "    try:\n",
    "        if isinstance(tensor, torch.Tensor):\n",
    "            return tensor.detach().cpu().numpy()\n",
    "        elif isinstance(tensor, np.ndarray):\n",
    "            return tensor\n",
    "        else:\n",
    "            return np.array(tensor)\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting tensor to numpy: {e}\")\n",
    "        # Fallback method\n",
    "        try:\n",
    "            return np.array(tensor.detach().cpu().tolist())\n",
    "        except:\n",
    "            return np.array([])\n",
    "\n",
    "\n",
    "def visualize_predictions(model, data_loader, device, num_samples=5, confidence_threshold=0.5, save_dir='./validation_results'):\n",
    "    \"\"\"Visualize model predictions on validation set\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Class names and colors\n",
    "    classes = [\n",
    "        'background', 'ship', 'storage_tank', 'baseball_diamond', \n",
    "        'tennis_court', 'basketball_court', 'ground_track_field',\n",
    "        'bridge', 'large_vehicle', 'small_vehicle', 'helicopter',\n",
    "        'swimming_pool', 'roundabout', 'soccer_ball_field', \n",
    "        'plane', 'harbor'\n",
    "    ]\n",
    "    \n",
    "    class_colors = [\n",
    "        (0, 0, 0), (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0),\n",
    "        (255, 0, 255), (0, 255, 255), (128, 0, 0), (0, 128, 0), (0, 0, 128),\n",
    "        (128, 128, 0), (128, 0, 128), (0, 128, 128), (255, 128, 0), (255, 0, 128), (128, 255, 0)\n",
    "    ]\n",
    "    \n",
    "    sample_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, targets, original_imgs, filenames) in enumerate(data_loader):\n",
    "            if sample_count >= num_samples:\n",
    "                break\n",
    "                \n",
    "            if len(images) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Move to device\n",
    "            images_gpu = [img.to(device) for img in images]\n",
    "            \n",
    "            # Get predictions\n",
    "            predictions = model(images_gpu)\n",
    "            \n",
    "            # Process each image in the batch\n",
    "            for i in range(len(images)):\n",
    "                if sample_count >= num_samples:\n",
    "                    break\n",
    "                \n",
    "                # Get data\n",
    "                original_img = original_imgs[i]\n",
    "                filename = filenames[i]\n",
    "                target = targets[i]\n",
    "                pred = predictions[i]\n",
    "                \n",
    "                # Create visualization\n",
    "                fig, axes = plt.subplots(2, 2, figsize=(15, 15))\n",
    "                fig.suptitle(f'Sample: {filename}', fontsize=16)\n",
    "                \n",
    "                # 1. Original Image\n",
    "                axes[0, 0].imshow(original_img)\n",
    "                axes[0, 0].set_title('Original Image')\n",
    "                axes[0, 0].axis('off')\n",
    "                \n",
    "                # 2. Ground Truth\n",
    "                gt_img = original_img.copy()\n",
    "                gt_boxes = safe_tensor_to_numpy(target['boxes'])\n",
    "                gt_labels = safe_tensor_to_numpy(target['labels'])\n",
    "                gt_masks = safe_tensor_to_numpy(target['masks'])\n",
    "                \n",
    "                # Draw ground truth\n",
    "                axes[0, 1].imshow(gt_img)\n",
    "                for j, (box, label) in enumerate(zip(gt_boxes, gt_labels)):\n",
    "                    if label > 0:  # Skip background\n",
    "                        x1, y1, x2, y2 = box\n",
    "                        rect = Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                       linewidth=2, edgecolor='red', facecolor='none')\n",
    "                        axes[0, 1].add_patch(rect)\n",
    "                        axes[0, 1].text(x1, y1-5, f'GT: {classes[label]}', \n",
    "                                       color='red', fontsize=8, fontweight='bold')\n",
    "                \n",
    "                axes[0, 1].set_title(f'Ground Truth ({len(gt_boxes)} objects)')\n",
    "                axes[0, 1].axis('off')\n",
    "                \n",
    "                # 3. Predictions (Boxes)\n",
    "                pred_img = original_img.copy()\n",
    "                pred_boxes = safe_tensor_to_numpy(pred['boxes'])\n",
    "                pred_labels = safe_tensor_to_numpy(pred['labels'])\n",
    "                pred_scores = safe_tensor_to_numpy(pred['scores'])\n",
    "                \n",
    "                # Filter by confidence\n",
    "                valid_preds = pred_scores > confidence_threshold\n",
    "                pred_boxes = pred_boxes[valid_preds]\n",
    "                pred_labels = pred_labels[valid_preds]\n",
    "                pred_scores = pred_scores[valid_preds]\n",
    "                \n",
    "                axes[1, 0].imshow(pred_img)\n",
    "                for j, (box, label, score) in enumerate(zip(pred_boxes, pred_labels, pred_scores)):\n",
    "                    if label > 0:  # Skip background\n",
    "                        x1, y1, x2, y2 = box\n",
    "                        rect = Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                       linewidth=2, edgecolor='blue', facecolor='none')\n",
    "                        axes[1, 0].add_patch(rect)\n",
    "                        axes[1, 0].text(x1, y1-5, f'{classes[label]}: {score:.2f}', \n",
    "                                       color='blue', fontsize=8, fontweight='bold')\n",
    "                \n",
    "                axes[1, 0].set_title(f'Predictions ({len(pred_boxes)} objects, conf>{confidence_threshold})')\n",
    "                axes[1, 0].axis('off')\n",
    "                \n",
    "                # 4. Masks Overlay\n",
    "                mask_overlay = original_img.copy().astype(np.float32)\n",
    "                pred_masks = safe_tensor_to_numpy(pred['masks'])\n",
    "                \n",
    "                # Handle empty predictions\n",
    "                if len(pred_masks) > 0 and len(valid_preds) > 0:\n",
    "                    pred_masks = pred_masks[valid_preds]\n",
    "                else:\n",
    "                    pred_masks = np.array([])\n",
    "                \n",
    "                # Overlay predicted masks\n",
    "                for j, (mask, label) in enumerate(zip(pred_masks, pred_labels)):\n",
    "                    if label > 0:\n",
    "                        mask_binary = (mask[0] > 0.5).astype(np.uint8)\n",
    "                        color = np.array(class_colors[label])\n",
    "                        \n",
    "                        # Create colored mask\n",
    "                        colored_mask = np.zeros_like(mask_overlay)\n",
    "                        colored_mask[mask_binary == 1] = color\n",
    "                        \n",
    "                        # Blend with original image\n",
    "                        mask_overlay = mask_overlay * 0.7 + colored_mask * 0.3\n",
    "                \n",
    "                mask_overlay = np.clip(mask_overlay, 0, 255).astype(np.uint8)\n",
    "                axes[1, 1].imshow(mask_overlay)\n",
    "                axes[1, 1].set_title('Predicted Masks Overlay')\n",
    "                axes[1, 1].axis('off')\n",
    "                \n",
    "                # Add legend\n",
    "                legend_elements = []\n",
    "                for label in np.unique(pred_labels):\n",
    "                    if label > 0:\n",
    "                        color = np.array(class_colors[label]) / 255.0\n",
    "                        legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                                        markerfacecolor=color, markersize=10, \n",
    "                                                        label=classes[label]))\n",
    "                \n",
    "                if legend_elements:\n",
    "                    axes[1, 1].legend(handles=legend_elements, loc='upper right', fontsize=8)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                \n",
    "                # Save visualization\n",
    "                save_path = os.path.join(save_dir, f'validation_{sample_count:03d}_{filename}')\n",
    "                plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "                plt.show()\n",
    "                \n",
    "                # Print statistics\n",
    "                print(f\"\\nSample {sample_count + 1}: {filename}\")\n",
    "                print(f\"Ground Truth: {len(gt_boxes)} objects\")\n",
    "                print(f\"Predictions: {len(pred_boxes)} objects (conf > {confidence_threshold})\")\n",
    "                \n",
    "                if len(pred_boxes) > 0:\n",
    "                    print(\"Predicted classes:\")\n",
    "                    for label, score in zip(pred_labels, pred_scores):\n",
    "                        print(f\"  - {classes[label]}: {score:.3f}\")\n",
    "                \n",
    "                sample_count += 1\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Check dependencies first\n",
    "    try:\n",
    "        print(\"Checking dependencies...\")\n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "        print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "        print(f\"NumPy version: {np.__version__}\")\n",
    "        print(f\"PIL version: {Image.__version__}\")\n",
    "        print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Dependency check failed: {e}\")\n",
    "        print(\"Please ensure all required packages are installed:\")\n",
    "        print(\"pip install torch torchvision numpy pillow matplotlib opencv-python\")\n",
    "        return\n",
    "    \n",
    "    # Configuration\n",
    "    config = {\n",
    "        'data_dir': '/Users/soumendusekharbhattacharjee/Downloads/iSAID_data/train',\n",
    "        'checkpoint_path': './checkpoints/maskrcnn_epoch_4.pth',  # Update this path\n",
    "        'batch_size': 1,\n",
    "        'num_workers': 0,\n",
    "        'num_classes': 16,\n",
    "        'num_samples': 5,  # Reduced for testing\n",
    "        'confidence_threshold': 0.3,  # Lowered threshold\n",
    "        'save_dir': './validation_results'\n",
    "    }\n",
    "    \n",
    "    # Check if checkpoint exists\n",
    "    if not os.path.exists(config['checkpoint_path']):\n",
    "        print(f\"Warning: Checkpoint not found at {config['checkpoint_path']}\")\n",
    "        print(\"Available checkpoints:\")\n",
    "        checkpoint_dir = os.path.dirname(config['checkpoint_path'])\n",
    "        if os.path.exists(checkpoint_dir):\n",
    "            for f in os.listdir(checkpoint_dir):\n",
    "                if f.endswith('.pth'):\n",
    "                    print(f\"  - {os.path.join(checkpoint_dir, f)}\")\n",
    "        else:\n",
    "            print(f\"Checkpoint directory {checkpoint_dir} does not exist\")\n",
    "        \n",
    "        # Ask user if they want to continue without checkpoint\n",
    "        response = input(\"Continue without loading checkpoint? (y/n): \")\n",
    "        if response.lower() != 'y':\n",
    "            return\n",
    "        config['checkpoint_path'] = None\n",
    "    \n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    try:\n",
    "        # Load dataset\n",
    "        print(\"Loading validation dataset...\")\n",
    "        transform = transforms.Compose([transforms.ToTensor()])\n",
    "        dataset = iSAIDDataset(config['data_dir'], transforms=transform)\n",
    "        \n",
    "        # Use a subset for validation\n",
    "        total_size = min(len(dataset), 20)  # Use last 20 samples as validation\n",
    "        val_indices = list(range(len(dataset) - total_size, len(dataset)))\n",
    "        val_dataset = torch.utils.data.Subset(dataset, val_indices)\n",
    "        \n",
    "        print(f\"Validation samples: {len(val_dataset)}\")\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=config['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=config['num_workers'],\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "        \n",
    "        # Test data loading\n",
    "        print(\"Testing data loading...\")\n",
    "        for i, (images, targets, original_imgs, filenames) in enumerate(val_loader):\n",
    "            print(f\"Loaded batch {i}: {len(images)} images\")\n",
    "            if i >= 1:  # Test first 2 batches\n",
    "                break\n",
    "        \n",
    "        # Load model\n",
    "        print(\"Loading model...\")\n",
    "        model = get_model(config['num_classes'], config['checkpoint_path'])\n",
    "        model.to(device)\n",
    "        \n",
    "        # Create save directory\n",
    "        os.makedirs(config['save_dir'], exist_ok=True)\n",
    "        \n",
    "        # Visualize predictions\n",
    "        print(\"Generating validation visualizations...\")\n",
    "        visualize_predictions(\n",
    "            model=model,\n",
    "            data_loader=val_loader,\n",
    "            device=device,\n",
    "            num_samples=config['num_samples'],\n",
    "            confidence_threshold=config['confidence_threshold'],\n",
    "            save_dir=config['save_dir']\n",
    "        )\n",
    "        \n",
    "        print(f\"Validation results saved to: {config['save_dir']}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        print(\"\\nTroubleshooting tips:\")\n",
    "        print(\"1. Make sure all dependencies are installed\")\n",
    "        print(\"2. Check if your data directory path is correct\")\n",
    "        print(\"3. Verify your checkpoint file exists\")\n",
    "        print(\"4. Try reducing num_samples if running out of memory\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606109f3-1fcf-4c9f-9f03-30e61d76a844",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
